{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sec_web_scraper.Downloader import Downloader\n",
    "from sec_web_scraper import get_document_given_link\n",
    "from sec_edgar_downloader import Downloader\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import sys\n",
    "import unicodedata\n",
    "from operator import itemgetter\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from stocksent import Sentiment\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pandas import json_normalize\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce84019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create request header\n",
    "headers = {'User-Agent': \"peter.j.1410@gmail.com\"}\n",
    "\n",
    "# get all companies data\n",
    "companyTickers = requests.get(\n",
    "    \"https://www.sec.gov/files/company_tickers.json\",\n",
    "    headers=headers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a156c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tickers for all public companies listed on USA exchanges\n",
    "# parse CIK // without leading zeros\n",
    "directCik = companyTickers.json()['0']['cik_str']\n",
    "\n",
    "# dictionary to dataframe\n",
    "companyData = pd.DataFrame.from_dict(companyTickers.json(),\n",
    "                                     orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cbfd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add leading zeros to CIK in order to perform future mappings\n",
    "companyData['cik_str'] = companyData['cik_str'].astype(\n",
    "                           str).str.zfill(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "companyData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db820f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to look for 8K filings for this companies and search for the bankruptcy section filings in this form\n",
    "# Create new downloader object\n",
    "#d = Downloader()\n",
    "\n",
    "# input the year range for filing data\n",
    "#d.build_index_sec(2016, 2022)\n",
    "\n",
    "# get the dataframe with all 8-K's filled in the range above\n",
    "#list_of_8k = d.find_files_by_type('8-K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f84f0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da383bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use concatenation to obtain the full link for the each row of the table\n",
    "sec_link = \"https://www.sec.gov/Archives/\"\n",
    "list_of_8k['Filename'] = sec_link + list_of_8k['Filename'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_8k = list_of_8k.drop(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f76e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a fuction to get the text from 8-K form with necessary items using regex\n",
    "def parse_8k_filing(link):\n",
    "    # retrieve the text file from SEC\n",
    "    def get_text(link):\n",
    "        page = requests.get(link, headers={'User-Agent': 'Mozilla'})\n",
    "        html = bs(page.content, \"lxml\")\n",
    "        text = html.get_text().replace(u'\\xa0', ' ').replace(\"\\t\", \" \").replace(\"\\x92\", \"'\").split(\"\\n\")\n",
    "        return(text)\n",
    "    # find items reported in 8-k\n",
    "    def get_items(text):\n",
    "        itemPattern = re.compile(\"^(Item\\s[1-9][\\.\\d]*)\")\n",
    "        value = list()\n",
    "        for line in text:\n",
    "            if itemPattern.search(line.strip()) is not None:\n",
    "                value.append(itemPattern.search(line.strip()).group(0))\n",
    "\n",
    "        return(value)\n",
    "    # get the text associated with the items\n",
    "    def get_data(file, items) :\n",
    "        dataList = list()\n",
    "        stop = re.compile(\"SIGNATURE\", re.IGNORECASE)\n",
    "        control = 0\n",
    "        itemPattern = re.compile(\"|\".join(([\"^\" + i for i in items])))\n",
    "        for line in file:\n",
    "            if control == 0:\n",
    "                if itemPattern.search(line) is not None:\n",
    "                    it = itemPattern.search(line).group(0)\n",
    "                    control = 1\n",
    "            else:\n",
    "                if itemPattern.search(line) is not None:\n",
    "                    dataList.append([it])\n",
    "                    it = itemPattern.search(line).group(0)\n",
    "                elif stop.search(line) is not None:\n",
    "                    dataList.append([it])\n",
    "                    break\n",
    "                \n",
    "        if dataList:\n",
    "            data = pd.DataFrame.from_dict(dataList)\n",
    "            data.columns = ['Element']\n",
    "            data['Element'] = data['Element'].replace('\\.','',regex=True)\n",
    "            data = data[data['Element'] == \"Item 103\"]\n",
    "            \n",
    "            if data.empty:\n",
    "                return(False)\n",
    "            else:\n",
    "                return(True)\n",
    "            \n",
    "        else: \n",
    "            pass\n",
    "        \n",
    "    # Alternative version to extract items text from 8-K files \n",
    "    # in which lines are not properly divided.\n",
    "    def get_data_alternative(file):\n",
    "        dataList = list()\n",
    "        fullText = \" \".join(file)\n",
    "        fullText = unicodedata.normalize(\"NFKD\", fullText).encode('ascii', 'ignore').decode('utf8')\n",
    "        itemPattern = re.compile(\"\\.\\s*(Item\\s[1-9][\\.\\d]*)\")\n",
    "        items = itemPattern.findall(fullText)\n",
    "        itemsStart = list()\n",
    "        stop = re.compile(\"SIGNATURE\", re.IGNORECASE)\n",
    "        sig = stop.search(fullText).start()\n",
    "        for i in items:\n",
    "            itStartPattern = re.compile(\"\\.\\s*\"+i)\n",
    "            itemsStart.append(itStartPattern.search(fullText).start())\n",
    "        itemsStart.append(sig)\n",
    "        n = 1\n",
    "        while n < len(itemsStart) :\n",
    "            dataList.append([items[(n-1)]])\n",
    "            n += 1\n",
    "            \n",
    "        if dataList:\n",
    "            data = pd.DataFrame.from_dict(dataList)\n",
    "            data.columns = ['Element']\n",
    "            data['Element'] = data['Element'].replace('\\.','',regex=True)\n",
    "            data = data[data['Element'] == \"Item 103\"]\n",
    "            \n",
    "            if data.empty:\n",
    "                return(False)\n",
    "            else:\n",
    "                return(True)\n",
    "        else:\n",
    "            pass\n",
    "          \n",
    "    \n",
    "    file = get_text(link)\n",
    "    items = get_items(file)\n",
    "    if len(items) >= 1:      \n",
    "        return get_data(file, items)  \n",
    "    else:\n",
    "        try:\n",
    "             return get_data_alternative(file)\n",
    "        except:\n",
    "            return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09981289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dwownload the forms with the 1.03 Bankruptcy or Receivership section filled\n",
    "dl = Downloader()\n",
    "\n",
    "equity_ids = companyData['ticker']\n",
    "for equity_id in equity_ids:\n",
    "    dl.get(\"8-K\", equity_id, amount=1, query=\"Bankruptcy or Receivership Chapter 11\", after=\"2016-01-01\", before=\"2022-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fuction to get the financial data from 10-K form with all necessary elements of financial reports \n",
    "def get_financial_data(cik, t):\n",
    "    \n",
    "    t_before = t - 3\n",
    "    useful_facts = ['Assets', 'AccountsPayable', 'AssetsCurrent', 'AssetsNoncurrent', 'CashAndCashEquivalentsAtCarryingValue', 'EarningsPerShareBasic', 'GrossProfit', 'IncreaseDecreaseInAccountsPayable', 'InterestExpense', 'Liabilities', 'LiabilitiesAndStockholdersEquity', 'StockholdersEquity', 'LiabilitiesCurrent', 'LiabilitiesNoncurrent', 'LongTermDebt', 'NetCashProvidedByUsedInFinancingActivities', 'NetCashProvidedByUsedInInvestingActivities', 'NetCashProvidedByUsedInOperatingActivities', 'NetIncomeLoss', 'OperatingIncomeLoss', 'RetainedEarningsAccumulatedDeficit', 'Revenues']\n",
    "    \n",
    "    companyFacts = requests.get(\n",
    "        f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json',\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    financial_data = pd.DataFrame.from_dict((\n",
    "                companyFacts.json()['facts']['us-gaap'][useful_facts[0]]['units']['USD']))\n",
    "    financial_data = financial_data[financial_data['form'] == \"10-K\"]\n",
    "        \n",
    "    try:\n",
    "        financial_data = financial_data.drop(columns=['start', 'accn', 'form', 'frame', 'fp', 'fy'])\n",
    "    except KeyError:\n",
    "        financial_data = financial_data.drop(columns=['accn', 'form', 'frame', 'fp', 'fy'])\n",
    "        \n",
    "    financial_data['end'] = pd.to_datetime(financial_data['end'], infer_datetime_format=True)\n",
    "    financial_data['end'] = pd.to_numeric(financial_data['end'].dt.year)\n",
    "    financial_data = financial_data[financial_data['end'] == t_before]\n",
    "    financial_data = financial_data.sort_values('end').drop_duplicates('end',keep='last')\n",
    "    financial_data.rename(columns={'val' : useful_facts[0]}, inplace=True)\n",
    "    \n",
    "    for fact in useful_facts[1:]:\n",
    "\n",
    "        try: \n",
    "            fact_data = pd.DataFrame.from_dict((\n",
    "               companyFacts.json()['facts']['us-gaap'][fact]['units']['USD']))\n",
    "        except KeyError:\n",
    "            try:\n",
    "                fact_data = pd.DataFrame.from_dict((\n",
    "                    companyFacts.json()['facts']['us-gaap'][fact]['units']['USD/shares']))\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "        fact_data = fact_data[fact_data['form'] == \"10-K\"]\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            fact_data = fact_data.drop(columns=['start', 'accn', 'form', 'frame', 'fp', 'fy', 'filed'])\n",
    "        except KeyError:\n",
    "            fact_data = fact_data.drop(columns=['accn', 'form', 'frame', 'fp', 'fy', 'filed'])\n",
    "        \n",
    "        \n",
    "        fact_data['end'] = pd.to_datetime(fact_data['end'], infer_datetime_format=True)\n",
    "        fact_data['end'] = pd.to_numeric(fact_data['end'].dt.year)\n",
    "        fact_data = fact_data[fact_data['end'] == t_before]\n",
    "        fact_data = fact_data.sort_values('end').drop_duplicates('end',keep='last')\n",
    "        fact_data.rename(columns={'val' : fact}, inplace=True)\n",
    "        financial_data = financial_data.merge(fact_data, how='outer', on='end')\n",
    "        \n",
    "    financial_data['cik'] = cik\n",
    "    \n",
    "    return financial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cde121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to look for 10-K filings for this companies and search for the texts of different section filings in this form\n",
    "# Create new downloader object\n",
    "d = Downloader()\n",
    "\n",
    "# input the year range for filing data.\n",
    "d.build_index_sec(2014, 2022)\n",
    "\n",
    "# get the dataframe with all 8-K's filled in the range above\n",
    "list_of_10k = d.find_files_by_type('10-K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataframe with all 10's filled in the range above\n",
    "list_of_10k = d.find_files_by_type('10-K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cdb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add leading zeros to CIK in order to perform future mappings\n",
    "list_of_10k['CIK'] = list_of_10k['CIK'].astype(\n",
    "                           str).str.zfill(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850609a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ca677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data-transormation for the dataframe\n",
    "list_of_10k.drop(columns=['Company Name', 'Form Type', 'url'], inplace=True)\n",
    "list_of_10k = list_of_10k.merge(companyData, how='left', left_on='CIK', right_on='cik_str')\n",
    "list_of_10k.drop(columns=['cik_str','title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06396cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the companies mapped as a companies with financial distress\n",
    "fin_distress = pd.read_excel('financial_distress.xlsx')\n",
    "fin_distress['year'] = pd.to_numeric(fin_distress['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74add3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some additional transormation\n",
    "list_of_10k['Date Filed'] = pd.to_datetime(list_of_10k['Date Filed'], infer_datetime_format=True)\n",
    "list_of_10k['year'] = list_of_10k['Date Filed'].dt.year\n",
    "list_of_10k['year'] = pd.to_numeric(list_of_10k['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f491b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63933d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only companies with financial distress\n",
    "fin_distress_10k = fin_distress.merge(list_of_10k, how='left', on=['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc257d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get only the rows with the year before filling the 8-K form to obtain MD&A text\n",
    "fin_distress_10k = fin_distress_10k[fin_distress_10k['year_y'] == fin_distress_10k['year_x'] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the financial data for the companies in one dataframe\n",
    "financial_data_fin_distress = get_financial_data(fin_distress_10k['CIK'].iloc[0], fin_distress_10k['year_x'].iloc[0])\n",
    "\n",
    "for row in fin_distress_10k[1:].itertuples():\n",
    "    try:\n",
    "        buff = get_financial_data(row.CIK, row.year_x)\n",
    "        financial_data_fin_distress = pd.concat([financial_data_fin_distress, buff])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data_fin_distress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433331ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge witj initial dataframe to keep links to the forms\n",
    "fin_distress_10k = financial_data_fin_distress.merge(fin_distress_10k, how='left', left_on='cik', right_on='CIK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the full link for the 10-K forms\n",
    "sec_link = \"https://www.sec.gov/Archives/\"\n",
    "fin_distress_10k['Filename'] = sec_link + fin_distress_10k['Filename'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15877dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get the text of MD&A section of 10K form for each observation\n",
    "def parse_10k_filing(link, section):\n",
    "    \n",
    "    if section not in [0, 1, 2, 3]:\n",
    "        print(\"Not a valid section\")\n",
    "        sys.exit()\n",
    "    \n",
    "    def get_text(link):\n",
    "        page = requests.get(link, headers={'User-Agent': 'Mozilla'})\n",
    "        html = bs(page.content, \"lxml\")\n",
    "        text = html.get_text()\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode('ascii', 'ignore').decode('utf8')\n",
    "        text = text.split(\"\\n\")\n",
    "        text = \" \".join(text)\n",
    "        return(text)\n",
    "    \n",
    "    def extract_text(text, item_start, item_end):\n",
    "        item_start = item_start\n",
    "        item_end = item_end\n",
    "        starts = [i.start() for i in item_start.finditer(text)]\n",
    "        ends = [i.start() for i in item_end.finditer(text)]\n",
    "        positions = list()\n",
    "        for s in starts:\n",
    "            control = 0\n",
    "            for e in ends:\n",
    "                if control == 0:\n",
    "                    if s < e:\n",
    "                        control = 1\n",
    "                        positions.append([s,e])\n",
    "        item_length = 0\n",
    "        item_position = list()\n",
    "        for p in positions:\n",
    "            if (p[1]-p[0]) > item_length:\n",
    "                item_length = p[1]-p[0]\n",
    "                item_position = p\n",
    "\n",
    "        item_text = text[item_position[0]:item_position[1]]\n",
    "\n",
    "        return(item_text)\n",
    "\n",
    "    text = get_text(link)\n",
    "        \n",
    "    if section == 1 or section == 0:\n",
    "        try:\n",
    "            item1_start = re.compile(\"item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "            item1_end = re.compile(\"item\\s*1a[\\.\\;\\:\\-\\_]\\s*Risk|item\\s*2[\\.\\,\\;\\:\\-\\_]\\s*Prop\", re.IGNORECASE)\n",
    "            businessText = extract_text(text, item1_start, item1_end)\n",
    "        except:\n",
    "            businessText = \"Something went wrong!\"\n",
    "        \n",
    "    if section == 2 or section == 0:\n",
    "        try:\n",
    "            item1a_start = re.compile(\"(?<!,\\s)item\\s*1a[\\.\\;\\:\\-\\_]\\s*Risk\", re.IGNORECASE)\n",
    "            item1a_end = re.compile(\"item\\s*2[\\.\\;\\:\\-\\_]\\s*Prop|item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "            riskText = extract_text(text, item1a_start, item1a_end)\n",
    "        except:\n",
    "            riskText = \"Something went wrong!\"\n",
    "            \n",
    "    if section == 3 or section == 0:\n",
    "        try:\n",
    "            item7_start = re.compile(\"item\\s*[7][\\.\\;\\:\\-\\_]*\\s*\\\\bM\", re.IGNORECASE)\n",
    "            item7_end = re.compile(\"item\\s*7a[\\.\\;\\:\\-\\_]\\sQuanti|item\\s*8[\\.\\,\\;\\:\\-\\_]\\s*\", re.IGNORECASE)\n",
    "            mdaText = extract_text(text, item7_start, item7_end)\n",
    "        except:\n",
    "            mdaText = \"Something went wrong!\"\n",
    "    \n",
    "    if section == 0:\n",
    "        data = [businessText, riskText, mdaText]\n",
    "    elif section == 1:\n",
    "        data = [businessText]\n",
    "    elif section == 2:\n",
    "        data = [riskText]\n",
    "    elif section == 3:\n",
    "        data = [mdaText]\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text using the function above\n",
    "textual_data_fin_distress = pd.DataFrame(parse_10k_filing(fin_distress_10k['Filename'].iloc[0], 3), columns=['MD&A'])\n",
    "textual_data_fin_distress['cik'] = fin_distress_10k['cik'].iloc[0]\n",
    "\n",
    "for row in fin_distress_10k.itertuples():\n",
    "    buff = pd.DataFrame(parse_10k_filing(row.Filename, 3), columns=['MD&A'])\n",
    "    buff['cik'] = row.cik\n",
    "    textual_data_fin_distress = pd.concat([textual_data_fin_distress, buff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with initial dataframe\n",
    "final_fin_distress = fin_distress_10k.merge(textual_data_fin_distress, how='left', on='cik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse news from the financial news website using special library. We use a VADER here for obtaininh the polarity scores and get the average score for the period before filling 8-K form\n",
    "news_fin_distress = pd.DataFrame(data=(Sentiment(final_fin_distress['ticker'].iloc[0]).get_dataframe(days=4000)).mean()).T\n",
    "news_fin_distress['ticker'] = final_fin_distress['ticker'].iloc[0]\n",
    "\n",
    "# Get the headlines\n",
    "# Returns a DataFrame with headlines, source and sentiment scores.\n",
    "\n",
    "for row in final_fin_distress[1:].itertuples():\n",
    "    try:\n",
    "        buff = pd.DataFrame(data=(Sentiment(row.ticker).get_dataframe(days=4000)).mean()).T\n",
    "        buff['ticker'] = row.ticker\n",
    "        news_fin_distress = pd.concat([news_fin_distress, buff])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c95257",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_fin_distress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276612d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain final df with polarity scores for the financial news\n",
    "final_fin_distress = news_fin_distress.merge(final_fin_distress, how='left', on='ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc22f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the polarity score and subjectivity score for the MD&A\n",
    "final_fin_distress['MD&A'] = final_fin_distress['MD&A'].astype(str)\n",
    "final_fin_distress[['polarity', 'subjectivity']] = final_fin_distress['MD&A'].apply(lambda text: pd.Series(TextBlob(text).sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some additional columns for the dataframe with financial indicators\n",
    "final_fin_distress['D/E'] = final_fin_distress['Liabilities'] / final_fin_distress['StockholdersEquity']\n",
    "final_fin_distress['D/A'] = final_fin_distress['Liabilities'] / final_fin_distress['Assets']\n",
    "final_fin_distress['InterestCoverageRatio'] = final_fin_distress['OperatingIncomeLoss'] / final_fin_distress['InterestExpense']\n",
    "final_fin_distress['D/EBIT'] = final_fin_distress['Liabilities'] / final_fin_distress['OperatingIncomeLoss']\n",
    "final_fin_distress['CurrentRatio'] = final_fin_distress['AssetsCurrent'] / final_fin_distress['LiabilitiesCurrent']\n",
    "final_fin_distress['CashRatio'] = final_fin_distress['CashAndCashEquivalentsAtCarryingValue'] / final_fin_distress['LiabilitiesCurrent']\n",
    "final_fin_distress['ROE'] = final_fin_distress['NetIncomeLoss'] / final_fin_distress['StockholdersEquity']\n",
    "final_fin_distress['ROA'] = final_fin_distress['NetIncomeLoss'] / final_fin_distress['Assets']\n",
    "final_fin_distress['OperatingCashFlowRatio'] = final_fin_distress['NetCashProvidedByUsedInOperatingActivities'] / final_fin_distress['Liabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f85aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fin_distress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_fin_distress.to_excel('final_fin_distress.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to get 10-K forms again for the companies that will not be classified as companies with financial distress\n",
    "d = Downloader()\n",
    "d.build_index_sec(2014, 2022)\n",
    "list_of_10k = d.find_files_by_type('10-K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataframe with all 10's filled in the range above\n",
    "list_of_10k = d.find_files_by_type('10-K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb44b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add leading zeros to CIK in order to perform future mappings\n",
    "list_of_10k['CIK'] = list_of_10k['CIK'].astype(\n",
    "                           str).str.zfill(10)\n",
    "list_of_10k.drop(columns=['Company Name', 'Form Type', 'url'], inplace=True)\n",
    "list_of_10k = list_of_10k.merge(companyData, how='left', left_on='CIK', right_on='cik_str')\n",
    "list_of_10k.drop(columns=['cik_str','title'], inplace=True)\n",
    "list_of_10k['Date Filed'] = pd.to_datetime(list_of_10k['Date Filed'], infer_datetime_format=True)\n",
    "list_of_10k['year'] = list_of_10k['Date Filed'].dt.year\n",
    "list_of_10k['year'] = pd.to_numeric(list_of_10k['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d43000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the companies without the ticker\n",
    "list_of_10k = list_of_10k[list_of_10k['ticker'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the companies that had been already marked as financial distress\n",
    "list_of_10k_out = list_of_10k[~list_of_10k['ticker'].isin(fin_distress['ticker'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest information\n",
    "list_of_10k_out.sort_values('Date Filed').groupby('ticker').tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subsample of the data\n",
    "list_of_10k_out = list_of_10k_out.sample(n=500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdec0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_link = \"https://www.sec.gov/Archives/\"\n",
    "list_of_10k_out['Filename'] = sec_link + list_of_10k_out['Filename'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data_normal = get_financial_data(list_of_10k_out['CIK'].iloc[0], list_of_10k_out['year'].iloc[0])\n",
    "\n",
    "for row in list_of_10k_out[1:].itertuples():\n",
    "    try:\n",
    "        buff = get_financial_data(row.CIK, row.year)\n",
    "        financial_data_normal = pd.concat([financial_data_normal, buff])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10k_out = financial_data_normal.merge(list_of_10k_out, how='left', left_on='cik', right_on='CIK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10k_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting by first name\n",
    "list_of_10k_out.sort_values(\"cik\", inplace=True)\n",
    "  \n",
    "# dropping ALL duplicate values\n",
    "list_of_10k_out.drop_duplicates(subset=\"cik\",\n",
    "                     keep=False, inplace=True)\n",
    "  \n",
    "# displaying data\n",
    "list_of_10k_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_data_normal = pd.DataFrame(parse_10k_filing(list_of_10k_out['Filename'].iloc[0], 3), columns=['MD&A'])\n",
    "textual_data_normal['cik'] = list_of_10k_out['cik'].iloc[0]\n",
    "\n",
    "for row in list_of_10k_out[1:].itertuples():\n",
    "    buff = pd.DataFrame(parse_10k_filing(row.Filename, 3), columns=['MD&A'])\n",
    "    buff['cik'] = row.cik\n",
    "    textual_data_normal = pd.concat([textual_data_normal, buff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_data_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_normal = textual_data_normal.merge(list_of_10k_out, how='left', on='cik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_normal = pd.DataFrame(data=(Sentiment(final_normal['ticker'].iloc[2]).get_dataframe(days=4000)).mean()).T\n",
    "news_normal['ticker'] = final_normal['ticker'].iloc[2]\n",
    "\n",
    "# Get the headlines\n",
    "# Returns a DataFrame with headlines, source and sentiment scores.\n",
    "\n",
    "for row in final_normal.itertuples():\n",
    "    try:\n",
    "        buff = pd.DataFrame(data=(Sentiment(row.ticker).get_dataframe(days=4000)).mean()).T\n",
    "        buff['ticker'] = row.ticker\n",
    "        news_normal = pd.concat([news_normal, buff])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_normal = news_normal.merge(final_normal, how='left', on='ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "final_normal['MD&A'] = final_normal['MD&A'].astype(str)\n",
    "final_normal[['polarity', 'subjectivity']] = final_normal['MD&A'].apply(lambda text: pd.Series(TextBlob(text).sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016405fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_normal['D/E'] = final_normal['Liabilities'] / final_normal['StockholdersEquity']\n",
    "final_normal['D/A'] = final_normal['Liabilities'] / final_normal['Assets']\n",
    "final_normal['InterestCoverageRatio'] = final_normal['OperatingIncomeLoss'] / final_normal['InterestExpense']\n",
    "final_normal['D/EBIT'] = final_normal['Liabilities'] / final_normal['OperatingIncomeLoss']\n",
    "final_normal['CurrentRatio'] = final_normal['AssetsCurrent'] / final_normal['LiabilitiesCurrent']\n",
    "final_normal['CashRatio'] = final_normal['CashAndCashEquivalentsAtCarryingValue'] / final_normal['LiabilitiesCurrent']\n",
    "final_normal['ROE'] = final_normal['NetIncomeLoss'] / final_normal['StockholdersEquity']\n",
    "final_normal['ROA'] = final_normal['NetIncomeLoss'] / final_normal['Assets']\n",
    "final_normal['OperatingCashFlowRatio'] = final_normal['NetCashProvidedByUsedInOperatingActivities'] / final_normal['Liabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc45d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_normal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
